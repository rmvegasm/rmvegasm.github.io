[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Notes",
    "section": "",
    "text": "Python\n\n\ndsml\n\n\nnotes\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nwordcloud\n\n\nrtweet\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nschron\n\n\npoisson process\n\n\nchronologies\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2021\n\n\nRodrigo M. Vega\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/python_basics/index.html",
    "href": "blog/posts/python_basics/index.html",
    "title": "Basic data structures in Python",
    "section": "",
    "text": "Here I’ll explore the basic data structures in Python, how to create and manipulate them and how they differ from one another."
  },
  {
    "objectID": "blog/posts/python_basics/index.html#lists",
    "href": "blog/posts/python_basics/index.html#lists",
    "title": "Basic data structures in Python",
    "section": "Lists",
    "text": "Lists\nA list is an ordered collection of objects:\n\na_list = [1, 2, 3, 4, 5]\nprint(a_list)\n\nanother_list = [5, 4, 3, 2, 1]\nprint(another_list)\n\n# order matters:\nprint(a_list == another_list)\n\n[1, 2, 3, 4, 5]\n[5, 4, 3, 2, 1]\nFalse\n\n\nObjects within a list can be of different types:\n\nmixed_list = [1, 2, 'a', 'b', True, False]\nfor item in mixed_list:\n        print(f\"{item} is of type\", type(item))\n\n1 is of type <class 'int'>\n2 is of type <class 'int'>\na is of type <class 'str'>\nb is of type <class 'str'>\nTrue is of type <class 'bool'>\nFalse is of type <class 'bool'>\n\n\nIndexing is called slicing in python. I think there’s a lot in this name since the logic behind it is more related to slicing between elements than to retrieving a specified element. We slice using the syntax list[from = 0: to = len(.) - 1: step = 1]:\n\n# the first two elements\nprint(a_list[0:2])\n\n# every other element\nprint(a_list[::2])\n\n[1, 2]\n[1, 3, 5]\n\n\nIt also works with negative indexes, counting from the last element backwards:\n\n# without the last element\nprint(a_list[0:-1])\n\n# without the last 3\nprint(a_list[0:-3])\n\n# the same using default value\nprint(a_list[:-3])\n\n# in reverse order\nprint(a_list[::-1])\n\n[1, 2, 3, 4]\n[1, 2]\n[1, 2]\n[5, 4, 3, 2, 1]\n\n\nThe element sliced is always that one to the right of the the index. Here’s the logic:\npositive:  0...1...2...3...5...6\nvalues:    | 1 | 2 | 3 | 4 | 5 |\nnegative: -5..-4..-3..-2..-1...0\nIf we use a single number without :, we get the element istself instead of another list:\n\n# the second element as a list\nprint(a_list[1:2], type(a_list[1:2]))\n\n# the second element istself\nprint(a_list[1], type(a_list[1]))\n\n[2] <class 'list'>\n2 <class 'int'>\n\n\n\nModification and Arithmetic\nWe can assign a single element using indexes, or a bunch of them by slicing and providing a list as replacement:\n\n# modify an element\na_list[2] = 99\nprint(a_list)\n\n# modify a bunch of elements\na_list[3:5] = [11, 17]\nprint(a_list)\n\n[1, 2, 99, 4, 5]\n[1, 2, 99, 11, 17]\n\n\nSince lists can hold elements of different types, there’s no arithmetic on them. Operands will work differently or not at all. For instance, lists can be concatenated with the + operand:\n\n# `+` concatenates, as with strings\nprint(a_list + another_list)\n\n[1, 2, 99, 11, 17, 5, 4, 3, 2, 1]"
  },
  {
    "objectID": "blog/posts/python_basics/index.html#sets-and-dictionaries",
    "href": "blog/posts/python_basics/index.html#sets-and-dictionaries",
    "title": "Basic data structures in Python",
    "section": "Sets and Dictionaries",
    "text": "Sets and Dictionaries\nSets and dictionaries are both constructed using curly braces {}. A set is an unordered collection of unique elements that can be of different types:\n\n# a collection\na_set = {1, 2, 3, 4, 5}\nprint(a_set)\n\n# there is no order\nprint(a_set == {5, 4, 3, 2, 1})\n\n# and mixed type elements\nmixed_set = {1, 2, 3, 'a', True}\nprint(mixed_set)\n\n{1, 2, 3, 4, 5}\nTrue\n{'a', 1, 2, 3}\n\n\nRepeated elements are ignored, elements must be unique:\n\nprint({1, 2, 2, 3, 4, 4, 5})\nprint(a_set == {1, 2, 2, 5, 4, 3})\n\n{1, 2, 3, 4, 5}\nTrue\n\n\nA dictionary is a collection of key : value pairs:\n\na_dict = {'a' : 1, 'b' : 2, 'c' : 3}\nprint(a_dict)\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\nAs sets, dictionaries can hold different kinds of objects. Unlike sets, dictionaries can be subsetted (and modified) by the key:\n\n# get 'a'\nprint('element \"a\" is', a_dict['a'])\n\n# change 'a'\na_dict['a'] = 'banana'\nprint('but now it is', a_dict['a'])\n\nelement \"a\" is 1\nbut now it is banana"
  },
  {
    "objectID": "blog/posts/python_basics/index.html#tuples",
    "href": "blog/posts/python_basics/index.html#tuples",
    "title": "Basic data structures in Python",
    "section": "Tuples",
    "text": "Tuples\nTuples are oredered, inmutable collections of elements. They can hold any type of object, can be indexed and sliced:\n\na_tuple = (1, 2, True, [1, 2])\n\n# indexing\nprint(a_tuple[2])\n\n# slicing\nprint(a_tuple[::2])\n\nTrue\n(1, True)\n\n\nThey cannot be modified, but mutable elements within can be:\n\n# modify the first element on the list\na_tuple[3][0] = 99\nprint(a_tuple)\n\n(1, 2, True, [99, 2])"
  },
  {
    "objectID": "blog/posts/wordcloud/index.html",
    "href": "blog/posts/wordcloud/index.html",
    "title": "A rudimentary wordcloud from twitter data",
    "section": "",
    "text": "In this post I’ll build a wordcloud from twitter texts. I’ll be using the amazing rtweet package to access the twitter API. At the time of writing, rtweet can only acces the version 1 of the API, from which there is possible to obtain a single table with the query results. Version 2 allows for much more control on the query output but is not yet implemented in rtweet and I wanted to try it out ;)\nThe wordcloud is a powerfull way to visualize word frequencies in a text and grasp something about the topics covered within. To build it we need a list of words and the frequency for each of them. There is much more than it seems to this, but as a first naive approximation one could just separate each document into single words and build a table from that. This is exactly what this post will cover."
  },
  {
    "objectID": "blog/posts/wordcloud/index.html#write-a-summary-and-conclusion-here",
    "href": "blog/posts/wordcloud/index.html#write-a-summary-and-conclusion-here",
    "title": "A rudimentary wordcloud from twitter data",
    "section": "Write a summary and conclusion here",
    "text": "Write a summary and conclusion here\n\n\n\n\nComplete Code\n\n## ----r------------------------------------------------------------------------\nlibrary('rtweet')\n# Authenticate the currently logged-in user and store credentials, this needs to\n# be done only once per machine:\n# rtweet::auth_setup_default()\n\n# Do not perform query if results are already stored:\ntbl_file = 'tweets_tbl.rds'\ntweets_file = 'tweets.csv'\n\nif (file.exists(tweets_file)) {\n  tweets = read.csv(tweets_file)\n} else {\n  tweets = rtweet::search_tweets(\n    'chile -filter:quote -filter:media lang:es',\n    n = 2000,\n    include_rts = FALSE,\n    retryonratelimit = TRUE\n  )\n  # let's save the tibble just in case and write a csv ommiting list cols\n  saveRDS(tweets, tbl_file)\n  tweets = tweets[, sapply(tweets, class) != 'list'] |> as.data.frame()\n  write.csv(tweets, tweets_file, row.names = FALSE)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n## ----r------------------------------------------------------------------------\nis_link = function (x) grepl('^[[:lower:]]+://.+', x)\nis_hashtag = function (x) grepl('^#.+', x)\nis_mention = function (x) grepl('^@.+', x)\nis_number = function (x) grepl('^[[:digit:]]+[[:punct:]]*$', x)\n\n\n\n\n\n\n\n\n\n\n## ----r------------------------------------------------------------------------\nextract_words = function (x) {\n  x = x[!is_link(x)]\n  x = x[!is_hashtag(x)]\n  x = x[!is_mention(x)]\n  x = x[!is_number(x)]\n  x = lapply(x, gsub, pattern = '[^[:alnum:]]', replacement = '') |> unlist()\n  x = x[x != '']\n  tolower(x)\n}\n\n\n\n\n## ----r------------------------------------------------------------------------\nclean_tweets = tweets[['full_text']] |>\n  strsplit(split = '[[:space:]+]') |>\n  lapply(extract_words)\n\nwords = lapply(clean_tweets, function (x) {\n  n = nchar(x)\n  x[n >= 5 & n <= 10]\n})\n\nword_bag = unlist(words)\n\n\n\n\n## ----r------------------------------------------------------------------------\nword_bag = local({\n  query = 'chile'\n  word_bag[!(word_bag %in% query)]\n})\n\n\n## ----r------------------------------------------------------------------------\nlibrary('wordcloud2')\nlibrary('wesanderson')\n\nwf = local({\n  wb = table(word_bag)\n  word = names(wb)\n  freq = as.numeric(wb)\n  data.frame(word, freq)[freq >= 10, ]\n})\n\n# Nice colors from 'The Darjeeling Express'\nclrs = rep(wesanderson::wes_palette('Darjeeling1', 5), length.out = nrow(wf))\n\nwordcloud2::wordcloud2(wf,\n  background = 'transparent',\n  color = clrs,\n  size = .3)"
  },
  {
    "objectID": "blog/posts/poisson_process/index.html",
    "href": "blog/posts/poisson_process/index.html",
    "title": "Exploring the Poisson Process for age-depth models",
    "section": "",
    "text": "This is what we need:\n\ncompute age probability distribution for any depth unit conditional on the age of the previous depth unit or the age of the next depth unit\ndo this for each possible age in the unit we are conditioning on, scale the probabilities by the probability of that particular age\nsum all resulting distributions (age-wise) and normalize\ncompare this distribution to the theoretical distribution from a poisson process\ndo the same using a calibrated 14C age distribution times the poisson prior\n\nIf this works fine we could propagate the 14C uncertainty up and down the age model without recurring to MCMC sampling.\n\nFive from four - first attempt\nLet’s try with a simple poisson process and see if we can compute the distribution for time \\(t=5\\) from \\(t=4\\):\n\nlambda = 20\nn_depth = 5\ngrid_size = (n_depth * lambda + 4 * sqrt(n_depth * lambda))\nage_seq = 0:grid_size\n\n# theoretical distributions\nnormalize = function (x) x / sum(x)\n\nfour_th = dpois(age_seq, lambda = 4 * lambda)\nfive_th = dpois(age_seq, lambda = 5 * lambda)\n\n# five from four\nfive_ff = {\n  tmp = sapply(age_seq, function (x) {\n    dpois(age_seq - x, lambda = lambda) * dpois(x, lambda = 4 * lambda)\n  })\n  tmp = rowSums(tmp)\n  tmp = normalize(tmp)\n  tmp\n}\nrm(tmp)\n\n# plot\nopar = par(no.readonly = TRUE)\npar(mfrow = c(3, 1))\nplot(age_seq, five_ff, type = 'h')\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n\nDifferences are pretty small, all.equal wont consider them after normalization (output from dpois doesn’t add up to one):\n\npar(mfrow = c(2, 1))\n\n# test equality\nidentical(five_ff, five_th)\n\n[1] FALSE\n\nall.equal(five_ff, five_th)\n\n[1] \"Mean relative difference: 6.401264e-05\"\n\nall.equal(five_ff, normalize(five_th))\n\n[1] TRUE\n\nplot(five_ff - five_th, type = 'b')\nplot(five_ff - normalize(five_th), type = 'b')\n\n\n\n\n\n\nLogspace\nThis seems more like a precission issue than an approximation one. Lets try using the logspace to deal with small numbers. We will first need a function to sum the log-probabilities instead of rowSums. I am taking this one form this stackoverflow answer. The function uses recursion to sum vectors. It expects a vector of log-probabilities:\n\n# a function to sum log-probabilities\nlog_sum = function (x) {\n  if (length(x) == 1) return(x)\n  if (length(x) == 2) return(max(x) + log1p(exp(-abs(diff(x)))))\n  this  = x[1:2]\n  other = x[-c(1:2)]\n  out = c(log_sum(this), other)\n  return(log_sum(out))\n}\n\nNow lets rewrite the whole computation using the log-probabilities instead:\n\n# five from four\nfive_ff = {\n  tmp = sapply(age_seq, function (x) {\n    dpois(age_seq - x, lambda = lambda, log = TRUE) + dpois(x, lambda = 4 * lambda, log = TRUE)\n  })\n  tmp = apply(tmp, 1, log_sum)\n  tmp = normalize(exp(tmp))\n  tmp\n}\nrm(tmp)\n\n# plot\npar(mfrow = c(3, 1))\nplot(age_seq, five_ff, type = 'h')\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n# test equality\npar(mfrow = c(2, 1))\nidentical(five_ff, five_th)\n\n[1] FALSE\n\nall.equal(five_ff, five_th)\n\n[1] \"Mean relative difference: 6.401264e-05\"\n\nall.equal(five_ff, normalize(five_th))\n\n[1] TRUE\n\nplot(five_ff - five_th, type = 'b')\nplot(five_ff - normalize(five_th), type = 'b')\n\n\n\n\nNo difference… bollocks\n\n\nConvolution (bad implementation?)\nLets use the convolution to get \\(t=5\\) as the sum of \\(t=4\\) and a poisson r.v. with \\(\\lambda = 1\\):\n\nfive_ff = sapply(age_seq, function (x) {\n  sum(dpois(x, lambda = 4 * lambda) * dpois(age_seq - x, lambda = lambda))\n  })\n\n# plot\npar(mfrow = c(3, 1))\nplot(age_seq, five_ff, type = 'h')\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n# test equality\npar(mfrow = c(2, 1))\nidentical(five_ff, five_th)\n\n[1] FALSE\n\nall.equal(five_ff, five_th)\n\n[1] \"Mean relative difference: 1.418433\"\n\nall.equal(five_ff, normalize(five_th))\n\n[1] \"Mean relative difference: 1.418479\"\n\nplot(five_ff - five_th, type = 'b')\nplot(five_ff - normalize(five_th), type = 'b')\n\n\n\n\n\n\nMelvin Dale’s convolution for discrete independent r.v.s\nNow let’s try Melvin Dale’s (1979) version for discrete random variables. Let \\(Z = X + Y\\), all r.v.s with pmfs \\(F_X; F_Y\\) and where \\(P(X = i) = a_i; P(Y = i) = b_i; P(Z = i) = c_i\\)\n\\[ c_i = a_0b_i + a_1b_{i-1} + ... + a_{i -1}b_i + a_ib_0\\]\n\nfive_ff = sapply(age_seq, function (x) {\n  sum(dpois(0:x, lambda = 4 * lambda) * rev(dpois(0:x, lambda = lambda)))\n  })\n\n# plot\npar(mfrow = c(3, 1))\nplot(age_seq, five_ff, type = 'h')\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n# test equality\npar(mfrow = c(2, 1))\nidentical(five_ff, five_th)\n\n[1] FALSE\n\nall.equal(five_ff, five_th)\n\n[1] TRUE\n\nall.equal(five_ff, normalize(five_th))\n\n[1] \"Mean relative difference: 6.401674e-05\"\n\nplot(five_ff - five_th, type = 'b')\nplot(five_ff - normalize(five_th), type = 'b')\n\n\n\n\nSame approximation now using the logspace:\n\nfive_ff = {\n  tmp = sapply(age_seq, function (x) {\n    log_sum(dpois(0:x, lambda = 4 * lambda, log = TRUE) +\n            rev(dpois(0:x, lambda = lambda, log = TRUE)))\n  })\n  exp(tmp)\n}\n\nrm(tmp)\n\n# plot\npar(mfrow = c(3, 1))\nplot(age_seq, five_ff, type = 'h')\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n# test equality\npar(mfrow = c(2, 1))\nidentical(five_ff, five_th)\n\n[1] FALSE\n\nall.equal(five_ff, five_th)\n\n[1] TRUE\n\nall.equal(five_ff, normalize(five_th))\n\n[1] \"Mean relative difference: 6.401674e-05\"\n\nplot(five_ff - five_th, type = 'b')\nplot(five_ff - normalize(five_th), type = 'b')\n\n\n\n# reset graphical parameters\npar(opar)\n\nAgain, bollocks… maybe these differences are not that important?\n\n\nA realistic example\nLet’s assume that the differences for the Melvin Dale in logspace approach are due to computational precission (this are the smallest) and explore how would this work for real data. At this point, we are only able to cumpute probabilities for following steps within the process (sums), yet we would need to be able to compute these backwards also. In order to do so we need to learn how to cumpute the probability distribution for a difference.\nFrom Melvin Dale, the pdf of the difference \\(Z = X - Y\\) is also the fourier convolution of the pdfs \\(F_X\\) and \\(F_Y\\):\n\\[ P(Z = k) = \\sum F_X(k + y)F_Y(y) \\]\nIf we find an expansion form for this similar to the one we used for the sum of two r.v.s we should be ok. Following the same logic and taking advantage of the discreteness of the r.v.s, say \\(Z = X - Y; P(X = i) = a_i; P(Y = i) = b_i; P(Z = i) = c_i\\). Here we are thinking of each random variable as a colection of values and associated probabilities such that \\(X = [x_0, x_1, ..., x_i, ..., x_n]; P(X = x_i = i) = a_i\\). We want to sum the probabilities associated with each pair of values for \\(X\\) and \\(Y\\) that would yield a particular \\(z_i\\):\n\\[\n\\begin{align}\n  c_0 = a_0b_0 + a_1b_1 + a_2b_2 + ... + a_nb_n \\\\\n  c_1 = a_1b_0 + a_2b_1 + a_3b_2 + ... + a_nb_{n - 1} \\\\\n  c_2 = a_2b_0 + a_3b_1 + a_4b_2 + ... + a_nb_{n - 2} \\\\\n  \\vdots \\\\\n  c_i = a_ib_0 + ... + a_nb_{n - i} \\\\\n  \\vdots \\\\\n  c_{n - 1} = a_{n - 1}b_0 + a_nb_1 \\\\\n  c_n = a_nb_0\n\\end{align}\n\\]\nLet’s try to estimate \\(t = 4\\) from \\(t = 5\\):\n\nfour_ff = {\n  tmp = sapply(age_seq, function (x) {\n    log_sum(dpois(x:grid_size, lambda = 5 * lambda, log = TRUE) +\n            dpois(0:(grid_size - x), lambda = lambda, log = TRUE))\n  })\n  exp(tmp)\n}\n\nrm(tmp)\n\n# plot\npar(mfrow = c(3, 1))\nplot(age_seq, five_th, type = 'h')\nplot(age_seq, four_ff, type = 'h')\nplot(age_seq, four_th, type = 'h')\n\n\n\n# test equality\npar(opar)\nidentical(four_ff, four_th)\n\n[1] FALSE\n\nall.equal(four_ff, four_th)\n\n[1] \"Mean relative difference: 0.195754\"\n\nall.equal(five_ff, normalize(five_th))\n\n[1] \"Mean relative difference: 6.401674e-05\"\n\nplot(four_ff - four_th, type = 'b')\n\n\n\n\nSeems like there is no way to estimate the previous step without incresing uncentainty… Maybe that’s ok, we shall try with a realistic example and see what happens."
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rodrigo M. Vega",
    "section": "",
    "text": "This is my personal site, feel free to wonder around ;)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a self-taught R programmer with a strong research background. I love learning, take pride in finding new and creative solutions to problems, fixing broken things and explaining complex ideas in very simple terms. I thrive by helping others."
  }
]